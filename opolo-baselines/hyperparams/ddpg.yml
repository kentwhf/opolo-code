#Walker2d-v2:
#  env_wrapper: utils.wrappers.TimeFeatureWrapper
#  n_timesteps: !!float 2e6
#  policy: 'MlpPolicy'
#  noise_type: 'adaptive-param'
#  noise_std: 0.287
#  memory_limit: 100000
#  normalize_observations: True
#  normalize_returns: False
#  gamma: 0.999
#  actor_lr: !!float 0.000527
#  batch_size: 256
#  random_exploration: 0.0
#  policy_kwargs: 'dict(layer_norm=True)'

Walker2d-v2:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  gamma: 0.99
  memory_limit: 1000000
  noise_type: 'normal'
  noise_std: 0.1
  batch_size: 256
  normalize_observations: True
  normalize_returns: False

HalfCheetah-v2:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  gamma: 0.99
  memory_limit: 1000000
  noise_type: 'normal'
  noise_std: 0.1
  batch_size: 256
  normalize_observations: True
  normalize_returns: False

Hopper-v2:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  gamma: 0.99
  memory_limit: 1000000
  noise_type: 'normal'
  noise_std: 0.1
  batch_size: 256
  normalize_observations: True
  normalize_returns: False

#Hopper-v2:
#  env_wrapper: utils.wrappers.TimeFeatureWrapper
#  n_timesteps: !!float 2e6
#  policy: 'MlpPolicy'
#  gamma: 0.98
#  memory_limit: 1000000
#  noise_type: 'ornstein-uhlenbeck'
#  noise_std: 0.652
#  batch_size: 256
#  actor_lr: 0.00156
#  critic_lr: 0.00156
#  normalize_observations: True
#  normalize_returns: False

Swimmer-v2:
   env_wrapper: utils.wrappers.TimeFeatureWrapper
   n_timesteps: !!float 2e6
   policy: 'MlpPolicy'
   gamma: 0.99
   buffer_size: 100000
   noise_type: 'normal'
   noise_std: 0.1
   batch_size: 256
   policy_kwargs: "dict(layers=[400, 300])"

